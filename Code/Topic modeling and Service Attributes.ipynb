{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and load data\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"three_sentiments.csv\")\n",
    "df['average_sentiment'] = (df['bert_sentiment'] + df['vader_sentiment'] + 0.5*df['textblob_sentiment'])/2.5\n",
    "\n",
    "data_neg = df[df['NPS'] < 6]\n",
    "data_pos = df[df['NPS'] > 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "import numpy as np\n",
    "\n",
    "def NMF_TM (dataset):\n",
    "    # Preparing the text data\n",
    "    text_data = dataset['translated_comment'].values\n",
    "\n",
    "    # TF-IDF Vectorization\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "    tfidf = tfidf_vectorizer.fit_transform(text_data)\n",
    "\n",
    "    # Applying NMF for Topic Modeling\n",
    "    nmf_model = NMF(n_components=22, random_state=42)  # Number of topics\n",
    "    nmf_topic = nmf_model.fit_transform(tfidf)\n",
    "\n",
    "    # Displaying the top words for each topic\n",
    "    def display_topics(model, feature_names, no_top_words):\n",
    "        for topic_idx, topic in enumerate(model.components_):\n",
    "            print(f\"Topic {topic_idx}:\")\n",
    "            print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "    no_top_words = 10\n",
    "    display_topics(nmf_model, tfidf_vectorizer.get_feature_names_out(), no_top_words)\n",
    "    dataset['topic'] = np.argmax(nmf_topic, axis=1)\n",
    "    # dataset.reset_index(inplace=True)\n",
    "    # Grouping the dataset by topic and calculating mean NPS and sentiment scores for each topic\n",
    "    topic_summary = dataset.groupby('topic').agg({\n",
    "        'NPS': 'mean',\n",
    "        'avg_sentiment': 'mean',\n",
    "        'ID': 'count'\n",
    "    }).rename(columns={'ID': 'responses_count'}).reset_index()\n",
    "\n",
    "    return topic_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "know did doesn didn just doing companies little right moment\n",
      "Topic 1:\n",
      "service customer poor years unfriendly just new friendly lousy good\n",
      "Topic 2:\n",
      "price reduce high increase compared increases offer year gouging best\n",
      "Topic 3:\n",
      "answer questions didn question clear called received waiting got previous\n",
      "Topic 4:\n",
      "prices high increase higher bills little quite current increased lowering\n",
      "Topic 5:\n",
      "time long took waiting takes wait process times response connection\n",
      "Topic 6:\n",
      "don like talk understand think pay advertise good bills ask\n",
      "Topic 7:\n",
      "contact personal possible telephone got employee like impossible poor essent\n",
      "Topic 8:\n",
      "contract new year years essent gas offer customers number old\n",
      "Topic 9:\n",
      "bad communication service good accessibility advice website help experiences support\n",
      "Topic 10:\n",
      "recommend company friends reason companies rarely essent services decide eon\n",
      "Topic 11:\n",
      "expensive think way eon bills compared getting electricity quite essent\n",
      "Topic 12:\n",
      "lower costs network fees fee price prices fixed charges customers\n",
      "Topic 13:\n",
      "experience eon essent good little personal short company far satisfied\n",
      "Topic 14:\n",
      "electricity cheaper gas providers supplier provider pay use grid little\n",
      "Topic 15:\n",
      "work doesn does website essent site app just like portal\n",
      "Topic 16:\n",
      "better communication information customers faster offer think companies price offers\n",
      "Topic 17:\n",
      "phone reach available difficult chat email days reached touch questions\n",
      "Topic 18:\n",
      "want employee speak just chat like way really didn touch\n",
      "Topic 19:\n",
      "question answered resolved didn response asked simple understood chatbot received\n",
      "Topic 20:\n",
      "energy company supplier essent suppliers good choose consumption talk companies\n",
      "Topic 21:\n",
      "meter invoice received reading information months didn eon did invoices\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-4ae14590a9a2>:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset['topic'] = np.argmax(nmf_topic, axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "satisfaction services overall absolute maximum term complete great long communication\n",
      "Topic 1:\n",
      "satisfied services essent completely just eon years company overall work\n",
      "Topic 2:\n",
      "good communication just information explanation experience contact cooperation advice prices\n",
      "Topic 3:\n",
      "friendly helpful employee lady knowledgeable phone staff help dealings correct\n",
      "Topic 4:\n",
      "problems eon essent solved years haven ve company works didn\n",
      "Topic 5:\n",
      "service nice excellent quality professional customer good provider online super\n",
      "Topic 6:\n",
      "ok invoice communication works online contact didn resolved came receive\n",
      "Topic 7:\n",
      "clear explanation information simple answer easy understandable answers website use\n",
      "Topic 8:\n",
      "time long invoice comes came took invoices waiting arrive pay\n",
      "Topic 9:\n",
      "fast processing simple communication uncomplicated correct reliable professional nice accurate\n",
      "Topic 10:\n",
      "helped kindly quickly employee neatly lot clearly correctly nice lady\n",
      "Topic 11:\n",
      "problem solved solving solution eon free solve immediately approach didn\n",
      "Topic 12:\n",
      "speed processing request communication reliability willingness professionalism simplicity quality clarity\n",
      "Topic 13:\n",
      "customer years essent long complaints ve approach new eon like\n",
      "Topic 14:\n",
      "quickly went resolved smoothly came handled solved didn moved happened\n",
      "Topic 15:\n",
      "know don say like didn think answer experience essent eon\n",
      "Topic 16:\n",
      "great help company nice just willingness got correct excellent pleasant\n",
      "Topic 17:\n",
      "happy eon just ve innogy make bills really did years\n",
      "Topic 18:\n",
      "quick easy response request action processing use contact handling solution\n",
      "Topic 19:\n",
      "far experience essent worked services going haven complaints gone ve\n",
      "Topic 20:\n",
      "fine just works went like think going spoke transcription didn\n",
      "Topic 21:\n",
      "price lower eon company electricity reliable energy prices supplier reduce\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-4ae14590a9a2>:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset['topic'] = np.argmax(nmf_topic, axis=1)\n"
     ]
    }
   ],
   "source": [
    "# Run it on positive and negative data\n",
    "neg = NMF_TM(data_neg)\n",
    "pos = NMF_TM(data_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>NPS</th>\n",
       "      <th>avg_sentiment</th>\n",
       "      <th>responses_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2.983914</td>\n",
       "      <td>4.846844</td>\n",
       "      <td>373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.981567</td>\n",
       "      <td>4.287044</td>\n",
       "      <td>217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3.156951</td>\n",
       "      <td>5.399914</td>\n",
       "      <td>223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2.255682</td>\n",
       "      <td>4.207262</td>\n",
       "      <td>352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3.045455</td>\n",
       "      <td>5.340006</td>\n",
       "      <td>220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>2.840951</td>\n",
       "      <td>4.842709</td>\n",
       "      <td>547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>2.973529</td>\n",
       "      <td>4.656176</td>\n",
       "      <td>340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>2.147368</td>\n",
       "      <td>4.195277</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>2.387931</td>\n",
       "      <td>4.482617</td>\n",
       "      <td>464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>2.072131</td>\n",
       "      <td>3.130514</td>\n",
       "      <td>305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>2.658228</td>\n",
       "      <td>4.444462</td>\n",
       "      <td>237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>2.679104</td>\n",
       "      <td>3.834257</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>3.375000</td>\n",
       "      <td>4.534424</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>3.797297</td>\n",
       "      <td>5.271658</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>3.021277</td>\n",
       "      <td>4.674051</td>\n",
       "      <td>329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>2.258845</td>\n",
       "      <td>4.331447</td>\n",
       "      <td>537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>3.206751</td>\n",
       "      <td>6.049787</td>\n",
       "      <td>237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>2.079511</td>\n",
       "      <td>4.108373</td>\n",
       "      <td>654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>2.091483</td>\n",
       "      <td>4.470294</td>\n",
       "      <td>317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>2.229885</td>\n",
       "      <td>4.183045</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>2.735294</td>\n",
       "      <td>5.175865</td>\n",
       "      <td>544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>2.395473</td>\n",
       "      <td>4.387600</td>\n",
       "      <td>2253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    topic       NPS  avg_sentiment  responses_count\n",
       "0       0  2.983914       4.846844              373\n",
       "1       1  1.981567       4.287044              217\n",
       "2       2  3.156951       5.399914              223\n",
       "3       3  2.255682       4.207262              352\n",
       "4       4  3.045455       5.340006              220\n",
       "5       5  2.840951       4.842709              547\n",
       "6       6  2.973529       4.656176              340\n",
       "7       7  2.147368       4.195277              285\n",
       "8       8  2.387931       4.482617              464\n",
       "9       9  2.072131       3.130514              305\n",
       "10     10  2.658228       4.444462              237\n",
       "11     11  2.679104       3.834257              134\n",
       "12     12  3.375000       4.534424              192\n",
       "13     13  3.797297       5.271658              148\n",
       "14     14  3.021277       4.674051              329\n",
       "15     15  2.258845       4.331447              537\n",
       "16     16  3.206751       6.049787              237\n",
       "17     17  2.079511       4.108373              654\n",
       "18     18  2.091483       4.470294              317\n",
       "19     19  2.229885       4.183045              174\n",
       "20     20  2.735294       5.175865              544\n",
       "21     21  2.395473       4.387600             2253"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>NPS</th>\n",
       "      <th>avg_sentiment</th>\n",
       "      <th>responses_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>9.536477</td>\n",
       "      <td>6.932941</td>\n",
       "      <td>2248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>9.348741</td>\n",
       "      <td>7.695614</td>\n",
       "      <td>2185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>9.100114</td>\n",
       "      <td>7.904831</td>\n",
       "      <td>1748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>9.093927</td>\n",
       "      <td>7.816172</td>\n",
       "      <td>1235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9.201183</td>\n",
       "      <td>6.477791</td>\n",
       "      <td>845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>9.277881</td>\n",
       "      <td>7.553877</td>\n",
       "      <td>1076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>9.324786</td>\n",
       "      <td>6.794746</td>\n",
       "      <td>585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>9.080036</td>\n",
       "      <td>7.117654</td>\n",
       "      <td>2224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>9.182631</td>\n",
       "      <td>5.946476</td>\n",
       "      <td>2683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>9.513223</td>\n",
       "      <td>6.975635</td>\n",
       "      <td>1210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>9.047705</td>\n",
       "      <td>7.451051</td>\n",
       "      <td>1111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>9.460856</td>\n",
       "      <td>6.277088</td>\n",
       "      <td>1239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>9.645552</td>\n",
       "      <td>6.904201</td>\n",
       "      <td>1405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>9.084287</td>\n",
       "      <td>6.993185</td>\n",
       "      <td>1661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>9.434991</td>\n",
       "      <td>6.688719</td>\n",
       "      <td>2292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>8.829206</td>\n",
       "      <td>5.648841</td>\n",
       "      <td>1171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>9.326279</td>\n",
       "      <td>8.211835</td>\n",
       "      <td>1134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>9.413514</td>\n",
       "      <td>8.447234</td>\n",
       "      <td>370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>9.415725</td>\n",
       "      <td>7.020052</td>\n",
       "      <td>2124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>9.146182</td>\n",
       "      <td>6.796275</td>\n",
       "      <td>1074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>9.199336</td>\n",
       "      <td>6.708051</td>\n",
       "      <td>903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>9.049681</td>\n",
       "      <td>6.347915</td>\n",
       "      <td>3764</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    topic       NPS  avg_sentiment  responses_count\n",
       "0       0  9.536477       6.932941             2248\n",
       "1       1  9.348741       7.695614             2185\n",
       "2       2  9.100114       7.904831             1748\n",
       "3       3  9.093927       7.816172             1235\n",
       "4       4  9.201183       6.477791              845\n",
       "5       5  9.277881       7.553877             1076\n",
       "6       6  9.324786       6.794746              585\n",
       "7       7  9.080036       7.117654             2224\n",
       "8       8  9.182631       5.946476             2683\n",
       "9       9  9.513223       6.975635             1210\n",
       "10     10  9.047705       7.451051             1111\n",
       "11     11  9.460856       6.277088             1239\n",
       "12     12  9.645552       6.904201             1405\n",
       "13     13  9.084287       6.993185             1661\n",
       "14     14  9.434991       6.688719             2292\n",
       "15     15  8.829206       5.648841             1171\n",
       "16     16  9.326279       8.211835             1134\n",
       "17     17  9.413514       8.447234              370\n",
       "18     18  9.415725       7.020052             2124\n",
       "19     19  9.146182       6.796275             1074\n",
       "20     20  9.199336       6.708051              903\n",
       "21     21  9.049681       6.347915             3764"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coherence Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(range(2, 11),\n",
       " [0.394961724744598,\n",
       "  0.43604674196968896,\n",
       "  0.39131220346762885,\n",
       "  0.4397453043724028,\n",
       "  0.407316580580849,\n",
       "  0.4405973959930031,\n",
       "  0.4650447109770488,\n",
       "  0.47632757287299426,\n",
       "  0.4695741054541352])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-importing necessary libraries and redefining required functions after code execution state reset\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.matutils import Sparse2Corpus\n",
    "import numpy as np\n",
    "\n",
    "# Loading the dataset again\n",
    "file_path = 'three_sentiments.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Preparing the text data\n",
    "text_data = data['translated_comment_preprocessed'].values\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(text_data)\n",
    "\n",
    "# Convert TF-IDF to Gensim format\n",
    "corpus = Sparse2Corpus(tfidf, documents_columns=False)\n",
    "id_map = dict((v, k) for k, v in tfidf_vectorizer.vocabulary_.items())\n",
    "dictionary = Dictionary.from_corpus(corpus, id2word=id_map)\n",
    "\n",
    "# Function to calculate coherence score for NMF\n",
    "def calculate_coherence_score(tfidf_vectorizer, tfidf, n_topics, texts, dictionary):\n",
    "    # Fit NMF model\n",
    "    nmf_model = NMF(n_components=n_topics, random_state=42)\n",
    "    W = nmf_model.fit_transform(tfidf)\n",
    "    H = nmf_model.components_\n",
    "    \n",
    "    # Create topics\n",
    "    topics = [[tfidf_vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-11:-1]] for topic in H]\n",
    "    \n",
    "    # Calculate Coherence\n",
    "    coherence_model = CoherenceModel(topics=topics, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "    return coherence_model.get_coherence()\n",
    "\n",
    "# Preparing texts for coherence calculation\n",
    "texts = [doc.split() for doc in data['translated_comment_preprocessed'].tolist()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Range of topics to evaluate\n",
    "n_topics_range = range(2, 101)\n",
    "coherence_scores = []\n",
    "\n",
    "for n_topics in n_topics_range:\n",
    "    coherence_score = calculate_coherence_score(tfidf_vectorizer, tfidf, n_topics, texts, dictionary)\n",
    "    coherence_scores.append(coherence_score)\n",
    "\n",
    "# Displaying the coherence scores for each number of topics\n",
    "coherence_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.394961724744598,\n",
       " 0.43604674196968896,\n",
       " 0.39131220346762885,\n",
       " 0.4397453043724028,\n",
       " 0.407316580580849,\n",
       " 0.4405973959930031,\n",
       " 0.4650447109770488,\n",
       " 0.47632757287299426,\n",
       " 0.4695741054541352,\n",
       " 0.4735428468011607,\n",
       " 0.4904712668978726,\n",
       " 0.49725634812279523,\n",
       " 0.5046920629027244,\n",
       " 0.5109240103407463,\n",
       " 0.5100003708019907,\n",
       " 0.5044256031500398,\n",
       " 0.5008038956081855,\n",
       " 0.5035056114086479,\n",
       " 0.5194522220440649,\n",
       " 0.5125026483508022,\n",
       " 0.5218825170345145,\n",
       " 0.5277660877483594,\n",
       " 0.5164933684893136,\n",
       " 0.5180214460913636,\n",
       " 0.5181022271765825,\n",
       " 0.5154812831305873,\n",
       " 0.5136278569341051,\n",
       " 0.4995457327954947,\n",
       " 0.5069793140080893,\n",
       " 0.49907494946693753,\n",
       " 0.5000046416635883,\n",
       " 0.49800641729564293,\n",
       " 0.501982117474703,\n",
       " 0.5035260443807442,\n",
       " 0.5030294779936528,\n",
       " 0.5010388734741908,\n",
       " 0.48837888798833495,\n",
       " 0.49355617403636187,\n",
       " 0.49307179549826874,\n",
       " 0.4951750449358819,\n",
       " 0.49638908053142267,\n",
       " 0.48916492829041874,\n",
       " 0.48032531943971885,\n",
       " 0.48083974910542643,\n",
       " 0.4895530052779714,\n",
       " 0.4799764091644315,\n",
       " 0.4726065170752123,\n",
       " 0.4840497830368351,\n",
       " 0.4738844533305256,\n",
       " 0.47273512068171525,\n",
       " 0.47239714075655753,\n",
       " 0.46898650060683844,\n",
       " 0.4689103918157064,\n",
       " 0.4724769823812517,\n",
       " 0.4717938139538612,\n",
       " 0.47410139165983867,\n",
       " 0.4726562802058844,\n",
       " 0.4762336556009215,\n",
       " 0.47222742965381115,\n",
       " 0.47448114659508916,\n",
       " 0.46977069422435996,\n",
       " 0.4736624605174341,\n",
       " 0.4640720012665907,\n",
       " 0.4619416689970211,\n",
       " 0.4652005741124878,\n",
       " 0.4590889078081961,\n",
       " 0.4650050789375066,\n",
       " 0.46087168929887257,\n",
       " 0.45862603593543777,\n",
       " 0.4572186296310055,\n",
       " 0.4638953685587709,\n",
       " 0.45361496910503624,\n",
       " 0.4546461037199071,\n",
       " 0.46195598237564267,\n",
       " 0.4529404054776017,\n",
       " 0.4491442493169391]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coherence_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Service Attributes Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Complex Noun Phrases: ['The customer service', 'the staff']\n"
     ]
    }
   ],
   "source": [
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Function to extract complex noun phrases using spaCy\n",
    "def extract_complex_noun_phrases(text):\n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Extract noun phrases\n",
    "    noun_phrases = [chunk.text for chunk in doc.noun_chunks]\n",
    "    \n",
    "    return noun_phrases\n",
    "\n",
    "# Sample customer comment\n",
    "sample_comment = \"The customer service was excellent, and the staff was very helpful and friendly.\"\n",
    "\n",
    "# Extract complex noun phrases from the comment\n",
    "attributes = extract_complex_noun_phrases(sample_comment)\n",
    "\n",
    "print(\"Extracted Complex Noun Phrases:\", attributes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_noun_phrases(comments):\n",
    "    noun_phrases = []\n",
    "    for doc in nlp.pipe(comments, batch_size=50):\n",
    "        for chunk in doc.noun_chunks:\n",
    "            noun_phrases.append(chunk.text.lower())  # Convert to lower case for consistent counting\n",
    "    return noun_phrases\n",
    "\n",
    "comments = df['translated_comment_preprocessed']\n",
    "\n",
    "# Extract noun phrases from all comments\n",
    "all_noun_phrases = extract_noun_phrases(comments)\n",
    "\n",
    "# Count the frequency of each noun phrase\n",
    "phrase_counts = Counter(all_noun_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Common Noun Phrases: ['everything', 'satisfaction', 'i', 'eon', 'problem', 'no problem', 'anything', 'nothing', 'speed', 'invoice', 'question', 'customer', 'employee', 'contract', 'email', 'someone', 'price', 'anyone', 'people', 'essent', 'something', 'good service', 'u', 'time', 'information', 'that', 'request', 'satisfied service', 'everyone', 'reliability', 'company', 'no one', 'answer question', 'service', 'phone', 'work', 'care', 'bill', 'long time', 'money', 'communication', 'touch', 'good communication', 'electricity', 'customer service', 'website', 'chat', 'place', 'thing', 'lady', 'no contact', 'client', 'contact', 'you', 'site', 'supplier', 'answer', 'good help', 'nee', 'professionalism', 'satisfied customer', 'fast processing', 'month', 'lower price', 'others', 'good experience', 'day', 'consumption', 'lot', 'chatbot', 'meter', 'no complaint', 'conversation', 'satisfaction service', 'internet', 'no answer question', 'account', 'app', 'satisfied eon', 'appointment', 'transcription', 'another supplier', 'new contract', 'new customer', 'connection', 'process', 'advice', 'clear explanation', 'waiting time', 'overall satisfaction', 'no reason', 'installation', 'good information', 'easy use', 'person', 'great help', 'great satisfaction', 'no answer', 'data', 'agreement']\n"
     ]
    }
   ],
   "source": [
    "# Identify the most common noun phrases\n",
    "most_common_phrases = phrase_counts.most_common(100)  # Adjust the number to get more or fewer phrases\n",
    "\n",
    "# Extract just the phrases into a list, excluding their counts\n",
    "most_common_phrases_list = [phrase for phrase, count in most_common_phrases]\n",
    "\n",
    "print(\"Most Common Noun Phrases:\", most_common_phrases_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loyalty_drivers = {\n",
    "    \"Service Quality\": [\"speed\", \"responsiveness\", \"customer service\", \"professionalism\", \"efficiency\", \"problem\", \"no problem\", \"service\", \"satisfied service\", \"satisfaction service\"],\n",
    "    \"Product Quality\": [\"reliability\", \"technology\"],\n",
    "    \"Value\": [\"price\", \"lower price\", \"value\", \"billing issues\"],\n",
    "    \"Customer Experience\": [\"satisfaction\", \"customer experience\", \"accessibility\", \"transparency\", \"invoice\"],\n",
    "    \"Communication\": [\"communication\", \"good communication\", \"information\", \"good information\"],\n",
    "}\n",
    "service_attributes = [\n",
    "    \"satisfaction\", \"problem\", \"no problem\", \"speed\", \"invoice\",\n",
    "    \"customer service\", \"communication\", \"good communication\", \"price\",\n",
    "    \"lower price\", \"reliability\", \"information\", \"good information\",\n",
    "    \"professionalism\", \"service\", \"satisfied service\", \"satisfaction service\",\n",
    "    \"customer experience\", \"efficiency\", \"accessibility\", \"transparency\",\n",
    "    \"responsiveness\", \"billing issues\", \"technology\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customer experience: 8.16\n",
      "efficiency: 7.35\n",
      "good communication: 6.58\n",
      "reliability: 5.97\n",
      "price: 5.27\n",
      "speed: 5.14\n",
      "responsiveness: 5.1\n",
      "good information: 4.73\n",
      "lower price: 4.39\n",
      "transparency: 4.38\n",
      "satisfaction: 4.29\n",
      "technology: 4.23\n",
      "no problem: 4.22\n",
      "communication: 4.14\n",
      "professionalism: 4.14\n",
      "information: 4.1\n",
      "problem: 4.06\n",
      "invoice: 4.06\n",
      "customer service: 4.05\n",
      "service: 3.96\n",
      "accessibility: 3.93\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "driver_sentiment = defaultdict(list)\n",
    "\n",
    "def classify_and_aggregate_feedback(row):\n",
    "    comment = row['translated_comment'].lower()  # Ensure matching in lowercase\n",
    "    sentiment = row['avg_sentiment']\n",
    "    \n",
    "    # for driver, attributes in loyalty_drivers.items():        # For categories\n",
    "    #     if any(attribute in comment for attribute in attributes):\n",
    "    #         driver_sentiment[driver].append(sentiment)\n",
    "    \n",
    "    for attribute in service_attributes:            # For each attribute\n",
    "        if attribute in comment:\n",
    "            driver_sentiment[attribute].append(sentiment)\n",
    "\n",
    "# Apply the function to each row in the DataFrame\n",
    "data_neg.apply(classify_and_aggregate_feedback, axis=1)\n",
    "\n",
    "# Calculate average sentiment for each loyalty driver\n",
    "average_sentiment_per_driver = {driver: sum(sentiments) / len(sentiments) if sentiments else None for driver, sentiments in driver_sentiment.items()}\n",
    "\n",
    "sorted_attributes_by_sentiment = sorted(average_sentiment_per_driver.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Display the sorted average sentiment for each service attribute\n",
    "for attribute, avg_sentiment in sorted_attributes_by_sentiment:\n",
    "    print(f\"{attribute}: {round(avg_sentiment, 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
